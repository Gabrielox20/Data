{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "import cv2\n",
    "import mediapipe as mp\n",
    "import torch\n",
    "import numpy as np\n",
    "from torchvision import transforms\n",
    "from torchvision.models import resnet18\n",
    "import torch.nn as nn\n",
    "\n",
    "# Definición del modelo\n",
    "class SignLanguageModel(nn.Module):\n",
    "    def __init__(self):\n",
    "        super(SignLanguageModel, self).__init__()\n",
    "        self.base_model = resnet18(pretrained=True)\n",
    "        self.base_model.conv1 = nn.Conv2d(1, 64, kernel_size=(7, 7), stride=(2, 2), padding=(3, 3), bias=False)\n",
    "        self.base_model.fc = nn.Linear(self.base_model.fc.in_features, 29)\n",
    "    \n",
    "    def forward(self, x):\n",
    "        return self.base_model(x)\n",
    "\n",
    "device = torch.device('cuda' if torch.cuda.is_available() else 'cpu')\n",
    "\n",
    "# Cargar el modelo entrenado\n",
    "model = SignLanguageModel().to(device)\n",
    "model.load_state_dict(torch.load('modelo_final.pth'))\n",
    "model.eval()\n",
    "\n",
    "# MediaPipe para detección de manos\n",
    "mp_hands = mp.solutions.hands\n",
    "\n",
    "def detect_and_crop_hand(image, margin=20):\n",
    "    with mp_hands.Hands(static_image_mode=True, max_num_hands=1, min_detection_confidence=0.5) as hands:\n",
    "        results = hands.process(cv2.cvtColor(image, cv2.COLOR_BGR2RGB))\n",
    "        if results.multi_hand_landmarks:\n",
    "            for hand_landmarks in results.multi_hand_landmarks:\n",
    "                h, w, c = image.shape\n",
    "                x_min = min([landmark.x for landmark in hand_landmarks.landmark]) * w\n",
    "                x_max = max([landmark.x for landmark in hand_landmarks.landmark]) * w\n",
    "                y_min = min([landmark.y for landmark in hand_landmarks.landmark]) * h\n",
    "                y_max = max([landmark.y for landmark in hand_landmarks.landmark]) * h\n",
    "                \n",
    "                # Añadir margen\n",
    "                x_min = max(0, int(x_min - margin))\n",
    "                x_max = min(w, int(x_max + margin))\n",
    "                y_min = max(0, int(y_min - margin))\n",
    "                y_max = min(h, int(y_max + margin))\n",
    "                \n",
    "                cropped_image = image[y_min:y_max, x_min:x_max]\n",
    "                return cropped_image\n",
    "        return None\n",
    "\n",
    "# Transformaciones de imagen\n",
    "transform = transforms.Compose([\n",
    "    transforms.ToPILImage(),\n",
    "    transforms.Resize((224, 224)),\n",
    "    transforms.Grayscale(),\n",
    "    transforms.ToTensor(),\n",
    "    transforms.Normalize(mean=[0.5], std=[0.5])\n",
    "])\n",
    "\n",
    "# Capturar y procesar imágenes de la cámara web\n",
    "cap = cv2.VideoCapture(0)\n",
    "\n",
    "while True:\n",
    "    ret, frame = cap.read()\n",
    "    if not ret:\n",
    "        break\n",
    "    \n",
    "    cropped_image = detect_and_crop_hand(frame)\n",
    "    \n",
    "    if cropped_image is not None:\n",
    "        input_tensor = transform(cropped_image).unsqueeze(0).to(device)\n",
    "        output = model(input_tensor)\n",
    "        _, predicted = torch.max(output, 1)\n",
    "        predicted_class = predicted.item()\n",
    "        \n",
    "        # Mostrar la clase predicha en la imagen\n",
    "        cv2.putText(frame, f'Predicted: {predicted_class}', (10, 30), cv2.FONT_HERSHEY_SIMPLEX, 1, (255, 0, 0), 2, cv2.LINE_AA)\n",
    "    \n",
    "    # Mostrar el frame\n",
    "    cv2.imshow('Sign Language Detection', frame)\n",
    "    \n",
    "    if cv2.waitKey(1) & 0xFF == ord('q'):\n",
    "        break\n",
    "\n",
    "cap.release()\n",
    "cv2.destroyAllWindows()\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Una foto"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "No se pudo capturar la imagen de la cámara.\n"
     ]
    }
   ],
   "source": [
    "import cv2\n",
    "import mediapipe as mp\n",
    "import torch\n",
    "import numpy as np\n",
    "from torchvision import transforms\n",
    "from torchvision.models import resnet18\n",
    "import torch.nn as nn\n",
    "import matplotlib.pyplot as plt\n",
    "import time\n",
    "\n",
    "# Definición del modelo\n",
    "class SignLanguageModel(nn.Module):\n",
    "    def __init__(self):\n",
    "        super(SignLanguageModel, self).__init__()\n",
    "        self.base_model = resnet18(pretrained=True)\n",
    "        self.base_model.conv1 = nn.Conv2d(1, 64, kernel_size=(7, 7), stride=(2, 2), padding=(3, 3), bias=False)\n",
    "        self.base_model.fc = nn.Linear(self.base_model.fc.in_features, 29)\n",
    "    \n",
    "    def forward(self, x):\n",
    "        return self.base_model(x)\n",
    "\n",
    "device = torch.device('cuda' if torch.cuda.is_available() else 'cpu')\n",
    "\n",
    "# Cargar el modelo entrenado\n",
    "model = SignLanguageModel().to(device)\n",
    "model.load_state_dict(torch.load('modelo_final.pth'))\n",
    "model.eval()\n",
    "\n",
    "# MediaPipe para detección de manos\n",
    "mp_hands = mp.solutions.hands\n",
    "\n",
    "def detect_and_crop_hand(image, margin=20):\n",
    "    with mp_hands.Hands(static_image_mode=True, max_num_hands=1, min_detection_confidence=0.5) as hands:\n",
    "        results = hands.process(cv2.cvtColor(image, cv2.COLOR_BGR2RGB))\n",
    "        if results.multi_hand_landmarks:\n",
    "            for hand_landmarks in results.multi_hand_landmarks:\n",
    "                h, w, c = image.shape\n",
    "                x_min = min([landmark.x for landmark in hand_landmarks.landmark]) * w\n",
    "                x_max = max([landmark.x for landmark in hand_landmarks.landmark]) * w\n",
    "                y_min = min([landmark.y for landmark in hand_landmarks.landmark]) * h\n",
    "                y_max = max([landmark.y for landmark in hand_landmarks.landmark]) * h\n",
    "                \n",
    "                # Añadir margen\n",
    "                x_min = max(0, int(x_min - margin))\n",
    "                x_max = min(w, int(x_max + margin))\n",
    "                y_min = max(0, int(y_min - margin))\n",
    "                y_max = min(h, int(y_max + margin))\n",
    "                \n",
    "                cropped_image = image[y_min:y_max, x_min:x_max]\n",
    "                return cropped_image\n",
    "        return None\n",
    "\n",
    "# Transformaciones de imagen\n",
    "transform = transforms.Compose([\n",
    "    transforms.ToPILImage(),\n",
    "    transforms.Resize((224, 224)),\n",
    "    transforms.Grayscale(),\n",
    "    transforms.ToTensor(),\n",
    "    transforms.Normalize(mean=[0.5], std=[0.5])\n",
    "])\n",
    "\n",
    "# Lista de etiquetas para las clases\n",
    "labels = list(\"ABCDEFGHIJKLMNOPQRSTUVWXYZ\") + [\"nothing\", \"space\"]\n",
    "\n",
    "# Capturar una sola imagen de la cámara web\n",
    "cap = cv2.VideoCapture(0)\n",
    "\n",
    "# Asegurarse de que la cámara esté abierta\n",
    "if not cap.isOpened():\n",
    "    print(\"Error: No se pudo abrir la cámara.\")\n",
    "else:\n",
    "    # Esperar brevemente para dar tiempo a la cámara a inicializarse\n",
    "    time.sleep(2)\n",
    "\n",
    "    ret, frame = cap.read()\n",
    "    cap.release()\n",
    "\n",
    "    if ret:\n",
    "        cropped_image = detect_and_crop_hand(frame)\n",
    "        \n",
    "        if cropped_image is not None:\n",
    "            input_tensor = transform(cropped_image).unsqueeze(0).to(device)\n",
    "            \n",
    "            # Mostrar la imagen procesada\n",
    "            plt.imshow(cv2.cvtColor(cropped_image, cv2.COLOR_BGR2RGB))\n",
    "            plt.title(\"Imagen procesada para predicción\")\n",
    "            plt.axis('off')\n",
    "            plt.show()\n",
    "            \n",
    "            # Hacer la predicción\n",
    "            output = model(input_tensor)\n",
    "            _, predicted = torch.max(output, 1)\n",
    "            predicted_class = predicted.item()\n",
    "            \n",
    "            # Mostrar la clase predicha\n",
    "            print(f'Predicted class: {labels[predicted_class]}')\n",
    "        else:\n",
    "            print(\"No se detectó ninguna mano en la imagen.\")\n",
    "    else:\n",
    "        print(\"No se pudo capturar la imagen de la cámara.\")\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.13"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
